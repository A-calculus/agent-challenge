Install Mastra
To get started with Mastra, you’ll need access to a large language model (LLM). By default, Mastra is set up to work with OpenAI 
, so you’ll need an API key to begin.
Mastra also supports other LLM providers. For a full list of supported models and setup instructions, see Model Providers.
Prerequisites
* Node.js v20.0 or higher
* An API key from a supported Model Provider
Install using the create-mastra CLI
Our CLI is the fastest way to get started with Mastra. Run the following command to start the interactive setup:
pnpm create mastra@latest
Install using CLI flags
You can also run the Mastra CLI in non-interactive mode by passing all required flags, for example:
npx create-mastra@latest --project-name hello-mastra --example --components tools,agents,workflows --llm openai
See the create-mastra documentation for a full list of available CLI options.
Add your API key
Add your API key to the .env file:
.env
OPENAI_API_KEY=<your-api-key>
This example uses OpenAI. Each LLM provider uses a unique name. See Model Capabilities for more information.
You can now launch the Mastra Development Server and test your agent using the Mastra Playground.
Install manually
The following steps will walk you through installing Mastra manually.
Create a new project
Create a new project and change directory:
mkdir hello-mastra && cd hello-mastra
Initialize a TypeScript project including the @mastra/core package:
npm init -y
 
npm install typescript tsx @types/node mastra@latest --save-dev
 
npm install @mastra/core@latest zod@^3 @ai-sdk/openai
Add the dev and build scripts to package.json:
package.json
{
  "scripts": {
    // ...
    "dev": "mastra dev",
    "build": "mastra build"
  }
}
Initialize TypeScript
Create a tsconfig.json file:
touch tsconfig.json
Add the following configuration:
tsconfig.json
{
  "compilerOptions": {
    "target": "ES2022",
    "module": "ES2022",
    "moduleResolution": "bundler",
    "esModuleInterop": true,
    "forceConsistentCasingInFileNames": true,
    "strict": true,
    "skipLibCheck": true,
    "noEmit": true,
    "outDir": "dist"
  },
  "include": [
    "src/**/*"
  ]
}
This TypeScript configuration is optimized for Mastra projects, using modern module resolution and strict type checking.
Set up your API key
Create .env file:
touch .env
Add your API key:
.env
OPENAI_API_KEY=<your-api-key>
This example uses OpenAI. Each LLM provider uses a unique name. See Model Capabilities for more information.
Create a Tool
Create a weather-tool.ts file:
mkdir -p src/mastra/tools && touch src/mastra/tools/weather-tool.ts
Add the following code:
src/mastra/tools/weather-tool.ts
import { createTool } from "@mastra/core/tools";
import { z } from "zod";
 
export const weatherTool = createTool({
  id: "get-weather",
  description: "Get current weather for a location",
  inputSchema: z.object({
    location: z.string().describe("City name")
  }),
  outputSchema: z.object({
    output: z.string()
  }),
  execute: async () => {
    return {
      output: "The weather is sunny"
    };
  }
});
See the full weatherTool example in Giving an Agent a Tool.
Create an Agent
Create a weather-agent.ts file:
mkdir -p src/mastra/agents && touch src/mastra/agents/weather-agent.ts
Add the following code:
src/mastra/agents/weather-agent.ts
import { openai } from "@ai-sdk/openai";
import { Agent } from "@mastra/core/agent";
import { weatherTool } from "../tools/weather-tool";
 
export const weatherAgent = new Agent({
  name: 'Weather Agent',
  instructions: `
      You are a helpful weather assistant that provides accurate weather information.
 
      Your primary function is to help users get weather details for specific locations. When responding:
      - Always ask for a location if none is provided
      - If the location name isn’t in English, please translate it
      - If giving a location with multiple parts (e.g. "New York, NY"), use the most relevant part (e.g. "New York")
      - Include relevant details like humidity, wind conditions, and precipitation
      - Keep responses concise but informative
 
      Use the weatherTool to fetch current weather data.
`,
  model: openai('gpt-4o-mini'),
  tools: { weatherTool }
});
Register the Agent
Create the Mastra entry point and register agent:
touch src/mastra/index.ts
Add the following code:
src/mastra/index.ts
import { Mastra } from "@mastra/core/mastra";
import { weatherAgent } from "./agents/weather-agent";
 
export const mastra = new Mastra({
  agents: { weatherAgent }
});
You can now launch the Mastra Development Server and test your agent using the Mastra Playground.
Add to an existing project
Mastra can be installed and integrated into a wide range of projects. Below are links to integration guides to help you get started:
* Next.js
* Vite + React
* Astro
* Express
mastra init
To install Mastra in an existing project, use the mastra init comman
Project Structure
This page provides a guide for organizing folders and files in Mastra. Mastra is a modular framework, and you can use any of the modules separately or together.
You could write everything in a single file, or separate each agent, tool, and workflow into their own files.
We don’t enforce a specific folder structure, but we do recommend some best practices, and the CLI will scaffold a project with a sensible structure.
Example Project Structure
A default project created with the CLI looks like this:
Src
Mastra
Agent
Tools
Workflows
index.ts
.Env
package.json
Tsconfig.json


op-level Folders
Folder
	Description
	src/mastra
	Core application folder
	src/mastra/agents
	Agent configurations and definitions
	src/mastra/tools
	Custom tool definitions
	src/mastra/workflows
	Workflow definitions
	Top-level Files
File
	Description
	src/mastra/index.ts
	Main configuration file for Mastra
	.env
	Environment variables
	package.json
	Node.js project metadata, scripts, and dependencies
	tsconfig.json
	TypeScript compiler configuration
	




Using Agents
Agents are one of the core Mastra primitives. Agents use a language model to decide on a sequence of actions. They can call functions (known as tools). You can compose them with workflows (the other main Mastra primitive), either by giving an agent a workflow as a tool, or by running an agent from within a workflow.
Agents can run autonomously in a loop, run once, or take turns with a user. You can give short-term, long-term, and working memory of their user interactions. They can stream text or return structured output (ie, JSON). They can access third-party APIs, query knowledge bases, and so on.
1. Creating an Agent
To create an agent in Mastra, you use the Agent class and define its properties:
src/mastra/agents/index.ts
import { Agent } from "@mastra/core/agent";
import { openai } from "@ai-sdk/openai";
 
export const myAgent = new Agent({
  name: "My Agent",
  instructions: "You are a helpful assistant.",
  model: openai("gpt-4o-mini"),
});
Note: Ensure that you have set the necessary environment variables, such as your OpenAI API key, in your .env file:
.env
OPENAI_API_KEY=your_openai_api_key
Also, make sure you have the @mastra/core package installed:
pnpm add @mastra/core@latest
Registering the Agent
Register your agent with Mastra to enable logging and access to configured tools and integrations:
src/mastra/index.ts
import { Mastra } from "@mastra/core";
import { myAgent } from "./agents";
 
export const mastra = new Mastra({
  agents: { myAgent },
});
2. Generating and streaming text
Generating text
Use the .generate() method to have your agent produce text responses:
src/mastra/index.ts
const response = await myAgent.generate([
  { role: "user", content: "Hello, how can you assist me today?" },
]);
 
console.log("Agent:", response.text);
For more details about the generate method and its options, see the generate reference documentation.
Streaming responses
For more real-time responses, you can stream the agent’s response:
src/mastra/index.ts
const stream = await myAgent.stream([
  { role: "user", content: "Tell me a story." },
]);
 
console.log("Agent:");
 
for await (const chunk of stream.textStream) {
  process.stdout.write(chunk);
}
For more details about streaming responses, see the stream reference documentation.
3. Structured Output
Agents can return structured data by providing a JSON Schema or using a Zod schema.
Using JSON Schema
const schema = {
  type: "object",
  properties: {
    summary: { type: "string" },
    keywords: { type: "array", items: { type: "string" } },
  },
  additionalProperties: false,
  required: ["summary", "keywords"],
};
 
const response = await myAgent.generate(
  [
    {
      role: "user",
      content:
        "Please provide a summary and keywords for the following text: ...",
    },
  ],
  {
    output: schema,
  },
);
 
console.log("Structured Output:", response.object);
Using Zod
You can also use Zod schemas for type-safe structured outputs.
First, install Zod:
pnpm add zod
Then, define a Zod schema and use it with the agent:
src/mastra/index.ts
import { z } from "zod";
 
// Define the Zod schema
const schema = z.object({
  summary: z.string(),
  keywords: z.array(z.string()),
});
 
// Use the schema with the agent
const response = await myAgent.generate(
  [
    {
      role: "user",
      content:
        "Please provide a summary and keywords for the following text: ...",
    },
  ],
  {
    output: schema,
  },
);
 
console.log("Structured Output:", response.object);
Using Tools
If you need to generate structured output alongside tool calls, you’ll need to use the experimental_output property instead of output. Here’s how:
const schema = z.object({
  summary: z.string(),
  keywords: z.array(z.string()),
});
 
const response = await myAgent.generate(
  [
    {
      role: "user",
      content:
        "Please analyze this repository and provide a summary and keywords...",
    },
  ],
  {
    // Use experimental_output to enable both structured output and tool calls
    experimental_output: schema,
  },
);
 
console.log("Structured Output:", response.object);


This allows you to have strong typing and validation for the structured data returned by the agent.
4. Multi-step tool use
Agents can be enhanced with tools - functions that extend their capabilities beyond text generation. Tools allow agents to perform calculations, access external systems, and process data. Agents not only decide whether to call tools they’re given, they determine the parameters that should be given to that tool.
For a detailed guide to creating and configuring tools, see the Adding Tools documentation, but below are the important things to know.
Using maxSteps
The maxSteps parameter controls the maximum number of sequential LLM calls an agent can make, particularly important when using tool calls. By default, it is set to 1 to prevent infinite loops in case of misconfigured tools. You can increase this limit based on your use case:
src/mastra/agents/index.ts
import { Agent } from "@mastra/core/agent";
import { openai } from "@ai-sdk/openai";
import * as mathjs from "mathjs";
import { z } from "zod";
 
export const myAgent = new Agent({
  name: "My Agent",
  instructions: "You are a helpful assistant that can solve math problems.",
  model: openai("gpt-4o-mini"),
  tools: {
    calculate: {
      description: "Calculator for mathematical expressions",
      schema: z.object({ expression: z.string() }),
      execute: async ({ expression }) => mathjs.evaluate(expression),
    },
  },
});
 
const response = await myAgent.generate(
  [
    {
      role: "user",
      content:
        "If a taxi driver earns $41 per hour and works 12 hours a day, how much do they earn in one day?",
    },
  ],
  {
    maxSteps: 5, // Allow up to 5 tool usage steps
  },
);
Streaming progress with onStepFinish
You can monitor the progress of multi-step operations using the onStepFinish callback. This is useful for debugging or providing progress updates to users.
onStepFinish is only available when streaming or generating text without structured output.
src/mastra/agents/index.ts
const response = await myAgent.generate(
  [{ role: "user", content: "Calculate the taxi driver's daily earnings." }],
  {
    maxSteps: 5,
    onStepFinish: ({ text, toolCalls, toolResults }) => {
      console.log("Step completed:", { text, toolCalls, toolResults });
    },
  },
);
Detecting completion with onFinish
The onFinish callback is available when streaming responses and provides detailed information about the completed interaction. It is called after the LLM has finished generating its response and all tool executions have completed. This callback receives the final response text, execution steps, token usage statistics, and other metadata that can be useful for monitoring and logging:
src/mastra/agents/index.ts
const stream = await myAgent.stream(
  [{ role: "user", content: "Calculate the taxi driver's daily earnings." }],
  {
    maxSteps: 5,
    onFinish: ({
      steps,
      text,
      finishReason, // 'complete', 'length', 'tool', etc.
      usage, // token usage statistics
      reasoningDetails, // additional context about the agent's decisions
    }) => {
      console.log("Stream complete:", {
        totalSteps: steps.length,
        finishReason,
        usage,
      });
    },
  },
);
5. Testing agents locally
Mastra provides a CLI command mastra dev to run your agents behind an API. By default, this looks for exported agents in files in the src/mastra/agents directory. It generates endpoints for testing your agent (eg http://localhost:4111/api/agents/myAgent/generate) and provides a visual playground where you can chat with an agent and view traces.
For more details, see the Local Dev Playground docs.
Next Steps
* Learn about Agent Memory in the Agent Memory guide.
* Learn about Agent Tools in the Agent Tools and MCP guide.
* See an example agent in the Chef Michel example.


Agent Memory
Agents in Mastra can leverage a powerful memory system to store conversation history, recall relevant information, and maintain persistent context across interactions. This allows agents to have more natural, stateful conversations.
Enabling Memory for an Agent
To enable memory, simply instantiate the Memory class and pass it to your agent’s configuration. You also need to install the memory package and a storage adapter:
pnpm add @mastra/memory@latest @mastra/libsql@latest
import { Agent } from "@mastra/core/agent";
import { Memory } from "@mastra/memory";
import { LibSQLStore } from "@mastra/libsql";
import { openai } from "@ai-sdk/openai";
 
const memory = new Memory({
  storage: new LibSQLStore({
    url: "file:../../memory.db",
  }),
});
 
const agent = new Agent({
  name: "MyMemoryAgent",
  instructions: "You are a helpful assistant with memory.",
  model: openai("gpt-4o"),
  memory, // Attach the memory instance
});
This basic setup uses the default settings. Visit the Memory documentation for more configuration info.
Using Memory in Agent Calls
To utilize memory during interactions, you must provide resourceId and threadId when calling the agent’s stream() or generate() methods.
* resourceId: Typically identifies the user or entity (e.g., user_123).
* threadId: Identifies a specific conversation thread (e.g., support_chat_456).
// Example agent call using memory
await agent.stream("Remember my favorite color is blue.", {
  resourceId: "user_alice",
  threadId: "preferences_thread",
});
 
// Later in the same thread...
const response = await agent.stream("What's my favorite color?", {
  resourceId: "user_alice",
  threadId: "preferences_thread",
});
// Agent will use memory to recall the favorite color.
These IDs ensure that conversation history and context are correctly stored and retrieved for the appropriate user and conversation.
Next Steps
Keep exploring Mastra’s memory capabilities like threads, conversation history, semantic recall, and working memory.


Using Tools with Agents
Tools are typed functions that can be executed by agents or workflows. Each tool has a schema defining its inputs, an executor function implementing its logic, and optional access to configured integrations.
Creating Tools
Here’s a basic example of creating a tool:
src/mastra/tools/weatherInfo.ts
import { createTool } from "@mastra/core/tools";
import { z } from "zod";
 
export const weatherInfo = createTool({
  id: "Get Weather Information",
  inputSchema: z.object({
    city: z.string(),
  }),
  description: `Fetches the current weather information for a given city`,
  execute: async ({ context: { city } }) => {
    // Tool logic here (e.g., API call)
    console.log("Using tool to fetch weather information for", city);
    return { temperature: 20, conditions: "Sunny" }; // Example return
  },
});
For details on creating and designing tools, see the Tools Overview.
Adding Tools to an Agent
To make a tool available to an agent, add it to the tools property in the agent’s configuration.
src/mastra/agents/weatherAgent.ts
import { Agent } from "@mastra/core/agent";
import { openai } from "@ai-sdk/openai";
import { weatherInfo } from "../tools/weatherInfo";
 
export const weatherAgent = new Agent({
  name: "Weather Agent",
  instructions:
    "You are a helpful assistant that provides current weather information. When asked about the weather, use the weather information tool to fetch the data.",
  model: openai("gpt-4o-mini"),
  tools: {
    weatherInfo,
  },
});
When you call the agent, it can now decide to use the configured tool based on its instructions and the user’s prompt.
Adding MCP Tools to an Agent
Model Context Protocol (MCP) 
provides a standardized way for AI models to discover and interact with external tools and resources. You can connect your Mastra agent to MCP servers to use tools provided by third parties.
For more details on MCP concepts and how to set up MCP clients and servers, see the MCP Overview.
Installation
First, install the Mastra MCP package:
pnpm add @mastra/mcp@latest
Using MCP Tools
Because there are so many MCP server registries to choose from, we’ve created an MCP Registry Registry 
to help you find MCP servers.
Once you have a server you want to use with your agent, import the Mastra MCPClient and add the server configuration.
src/mastra/mcp.ts
import { MCPClient } from "@mastra/mcp";
 
// Configure MCPClient to connect to your server(s)
export const mcp = new MCPClient({
  servers: {
    filesystem: {
      command: "npx",
      args: [
        "-y",
        "@modelcontextprotocol/server-filesystem",
        "/Users/username/Downloads",
      ],
    },
  },
});
Then connect your agent to the server tools:
src/mastra/agents/mcpAgent.ts
import { Agent } from "@mastra/core/agent";
import { openai } from "@ai-sdk/openai";
import { mcp } from "../mcp";
 
// Create an agent and add tools from the MCP client
const agent = new Agent({
  name: "Agent with MCP Tools",
  instructions: "You can use tools from connected MCP servers.",
  model: openai("gpt-4o-mini"),
  tools: await mcp.getTools(),
});
For more details on configuring MCPClient and the difference between static and dynamic MCP server configurations, see the MCP Overview.
Accessing MCP Resources
In addition to tools, MCP servers can also expose resources - data or content that can be retrieved and used in your application.
src/mastra/resources.ts
import { mcp } from "./mcp";
 
// Get resources from all connected MCP servers
const resources = await mcp.getResources();
 
// Access resources from a specific server
if (resources.filesystem) {
  const resource = resources.filesystem.find(
    (r) => r.uri === "filesystem://Downloads",
  );
  console.log(`Resource: ${resource?.name}`);
}
Each resource has a URI, name, description, and MIME type. The getResources() method handles errors gracefully - if a server fails or doesn’t support resources, it will be omitted from the results.
Accessing MCP Prompts
MCP servers can also expose prompts, which represent structured message templates or conversational context for agents.
Listing Prompts
src/mastra/prompts.ts
import { mcp } from "./mcp";
 
// Get prompts from all connected MCP servers
const prompts = await mcp.prompts.list();
 
// Access prompts from a specific server
if (prompts.weather) {
  const prompt = prompts.weather.find(
    (p) => p.name === "current"
  );
  console.log(`Prompt: ${prompt?.name}`);
}
Each prompt has a name, description, and (optional) version.
Retrieving a Prompt and Its Messages
src/mastra/prompts.ts
const { prompt, messages } = await mcp.prompts.get({ serverName: "weather", name: "current" });
console.log(prompt);    // { name: "current", version: "v1", ... }
console.log(messages);  // [ { role: "assistant", content: { type: "text", text: "..." } }, ... ]
Exposing Agents as Tools via MCPServer
In addition to using tools from MCP servers, your Mastra Agents themselves can be exposed as tools to any MCP-compatible client using Mastra’s MCPServer.
When an Agent instance is provided to an MCPServer configuration:
* It is automatically converted into a callable tool.
* The tool is named ask_<agentKey>, where <agentKey> is the identifier you used when adding the agent to the MCPServer’s agents configuration.
* The agent’s description property (which must be a non-empty string) is used to generate the tool’s description.
This allows other AI models or MCP clients to interact with your Mastra Agents as if they were standard tools, typically by “asking” them a question.
Example MCPServer Configuration with an Agent:
src/mastra/mcp.ts
import { Agent } from "@mastra/core/agent";
import { MCPServer } from "@mastra/mcp";
import { openai } from "@ai-sdk/openai";
import { weatherInfo } from "../tools/weatherInfo";
import { generalHelper } from "../agents/generalHelper";
 
const server = new MCPServer({
  name: "My Custom Server with Agent-Tool",
  version: "1.0.0",
  tools: {
    weatherInfo,
  },
  agents: { generalHelper }, // Exposes 'ask_generalHelper' tool
});
For an agent to be successfully converted into a tool by MCPServer, its description property must be set to a non-empty string in its constructor configuration. If the description is missing or empty, MCPServer will throw an error during initialization.


Adding Voice to Agents
Mastra agents can be enhanced with voice capabilities, allowing them to speak responses and listen to user input. You can configure an agent to use either a single voice provider or combine multiple providers for different operations.
Using a Single Provider
The simplest way to add voice to an agent is to use a single provider for both speaking and listening:
import { createReadStream } from "fs";
import path from "path";
import { Agent } from "@mastra/core/agent";
import { OpenAIVoice } from "@mastra/voice-openai";
import { openai } from "@ai-sdk/openai";
 
// Initialize the voice provider with default settings
const voice = new OpenAIVoice();
 
// Create an agent with voice capabilities
export const agent = new Agent({
  name: "Agent",
  instructions: `You are a helpful assistant with both STT and TTS capabilities.`,
  model: openai("gpt-4o"),
  voice,
});
 
// The agent can now use voice for interaction
const audioStream = await agent.voice.speak("Hello, I'm your AI assistant!", {
  filetype: "m4a",
});
 
playAudio(audioStream!);
 
try {
  const transcription = await agent.voice.listen(audioStream);
  console.log(transcription);
} catch (error) {
  console.error("Error transcribing audio:", error);
}
Using Multiple Providers
For more flexibility, you can use different providers for speaking and listening using the CompositeVoice class:
import { Agent } from "@mastra/core/agent";
import { CompositeVoice } from "@mastra/core/voice";
import { OpenAIVoice } from "@mastra/voice-openai";
import { PlayAIVoice } from "@mastra/voice-playai";
import { openai } from "@ai-sdk/openai";
 
export const agent = new Agent({
  name: "Agent",
  instructions: `You are a helpful assistant with both STT and TTS capabilities.`,
  model: openai("gpt-4o"),
 
  // Create a composite voice using OpenAI for listening and PlayAI for speaking
  voice: new CompositeVoice({
    input: new OpenAIVoice(),
    output: new PlayAIVoice(),
  }),
});
Working with Audio Streams
The speak() and listen() methods work with Node.js streams. Here’s how to save and load audio files:
Saving Speech Output
The speak method returns a stream that you can pipe to a file or speaker.
import { createWriteStream } from "fs";
import path from "path";
 
// Generate speech and save to file
const audio = await agent.voice.speak("Hello, World!");
const filePath = path.join(process.cwd(), "agent.mp3");
const writer = createWriteStream(filePath);
 
audio.pipe(writer);
 
await new Promise<void>((resolve, reject) => {
  writer.on("finish", () => resolve());
  writer.on("error", reject);
});
Transcribing Audio Input
The listen method expects a stream of audio data from a microphone or file.
import { createReadStream } from "fs";
import path from "path";
 
// Read audio file and transcribe
const audioFilePath = path.join(process.cwd(), "/agent.m4a");
const audioStream = createReadStream(audioFilePath);
 
try {
  console.log("Transcribing audio file...");
  const transcription = await agent.voice.listen(audioStream, {
    filetype: "m4a",
  });
  console.log("Transcription:", transcription);
} catch (error) {
  console.error("Error transcribing audio:", error);
}
Speech-to-Speech Voice Interactions
For more dynamic and interactive voice experiences, you can use real-time voice providers that support speech-to-speech capabilities:
import { Agent } from "@mastra/core/agent";
import { getMicrophoneStream } from "@mastra/node-audio";
import { OpenAIRealtimeVoice } from "@mastra/voice-openai-realtime";
import { search, calculate } from "../tools";
 
// Initialize the realtime voice provider
const voice = new OpenAIRealtimeVoice({
  apiKey: process.env.OPENAI_API_KEY,
  model: "gpt-4o-mini-realtime",
  speaker: "alloy",
});
 
// Create an agent with speech-to-speech voice capabilities
export const agent = new Agent({
  name: "Agent",
  instructions: `You are a helpful assistant with speech-to-speech capabilities.`,
  model: openai("gpt-4o"),
  tools: {
    // Tools configured on Agent are passed to voice provider
    search,
    calculate,
  },
  voice,
});
 
// Establish a WebSocket connection
await agent.voice.connect();
 
// Start a conversation
agent.voice.speak("Hello, I'm your AI assistant!");
 
// Stream audio from a microphone
const microphoneStream = getMicrophoneStream();
agent.voice.send(microphoneStream);
 
// When done with the conversation
agent.voice.close();
Event System
The realtime voice provider emits several events you can listen for:
// Listen for speech audio data sent from voice provider
agent.voice.on("speaking", ({ audio }) => {
  // audio contains ReadableStream or Int16Array audio data
});
 
// Listen for transcribed text sent from both voice provider and user
agent.voice.on("writing", ({ text, role }) => {
  console.log(`${role} said: ${text}`);
});
 
// Listen for errors
agent.voice.on("error", (error) => {
  console.error("Voice error:", error);
});
Supported Voice Providers
Mastra supports multiple voice providers for text-to-speech (TTS) and speech-to-text (STT) capabilities:
Provider
	Package
	Features
	Reference
	OpenAI
	@mastra/voice-openai
	TTS, STT
	Documentation
	OpenAI Realtime
	@mastra/voice-openai-realtime
	Realtime speech-to-speech
	Documentation
	ElevenLabs
	@mastra/voice-elevenlabs
	High-quality TTS
	Documentation
	PlayAI
	@mastra/voice-playai
	TTS
	Documentation
	Google
	@mastra/voice-google
	TTS, STT
	Documentation
	Deepgram
	@mastra/voice-deepgram
	STT
	Documentation
	Murf
	@mastra/voice-murf
	TTS
	Documentation
	Speechify
	@mastra/voice-speechify
	TTS
	Documentation
	Sarvam
	@mastra/voice-sarvam
	TTS, STT
	Documentation
	Azure
	@mastra/voice-azure
	TTS, STT
	Documentation
	Cloudflare
	@mastra/voice-cloudflare
	TTS
	Documentation
	Agent Runtime Context
Mastra provides runtime context, which is a system based on dependency injection that enables you to configure your agents and tools with runtime variables. If you find yourself creating several different agents that do very similar things, runtime context allows you to combine them into one agent.
Overview
The dependency injection system allows you to:
1. Pass runtime configuration variables to agents through a type-safe runtimeContext
2. Access these variables within tool execution contexts
3. Modify agent behavior without changing the underlying code
4. Share configuration across multiple tools within the same agent
Basic Usage
const agent = mastra.getAgent("weatherAgent");
 
// Define your runtimeContext's type structure
type WeatherRuntimeContext = {
  "temperature-scale": "celsius" | "fahrenheit"; // Fixed typo in "fahrenheit"
};
 
const runtimeContext = new RuntimeContext<WeatherRuntimeContext>();
runtimeContext.set("temperature-scale", "celsius");
 
const response = await agent.generate("What's the weather like today?", {
  runtimeContext,
});
 
console.log(response.text);
Using with REST API
Here’s how to dynamically set temperature units based on a user’s location using the Cloudflare CF-IPCountry header:
src/index.ts
import { Mastra } from "@mastra/core";
import { RuntimeContext } from "@mastra/core/di";
import { agent as weatherAgent } from "./agents/weather";
 
// Define RuntimeContext type with clear, descriptive types
type WeatherRuntimeContext = {
  "temperature-scale": "celsius" | "fahrenheit";
};
 
export const mastra = new Mastra({
  agents: {
    weather: weatherAgent,
  },
  server: {
    middleware: [
      async (c, next) => {
        const country = c.req.header("CF-IPCountry");
        const runtimeContext = c.get<WeatherRuntimeContext>("runtimeContext");
 
        // Set temperature scale based on country
        runtimeContext.set(
          "temperature-scale",
          country === "US" ? "fahrenheit" : "celsius",
        );
 
        await next(); // Don't forget to call next()
      },
    ],
  },
});
Creating Tools with Variables
Tools can access runtimeContext variables and must conform to the agent’s runtimeContext type:
import { createTool } from "@mastra/core/tools";
import { z } from "zod";
 
export const weatherTool = createTool({
  id: "getWeather",
  description: "Get the current weather for a location",
  inputSchema: z.object({
    location: z.string().describe("The location to get weather for"),
  }),
  execute: async ({ context, runtimeContext }) => {
    // Type-safe access to runtimeContext variables
    const temperatureUnit = runtimeContext.get("temperature-scale");
 
    const weather = await fetchWeather(context.location, {
      temperatureUnit,
    });
 
    return { result: weather };
  },
});
 
async function fetchWeather(
  location: string,
  { temperatureUnit }: { temperatureUnit: "celsius" | "fahrenheit" },
): Promise<WeatherResponse> {
  // Implementation of weather API call
  const response = await weatherApi.fetch(location, temperatureUnit);
 
  return {
    location,
    temperature: "72°F",
    conditions: "Sunny",
    unit: temperatureUnit,
  };
}
Dynamic Agents
Dynamic agents use runtime context, like user IDs and other important parameters, to adjust their settings in real-time.
This means they can change the model they use, update their instructions, and select different tools as needed.
By using this context, agents can better respond to each user’s needs. They can also call any API to gather more information, which helps improve what the agents can do.
Example Configuration
Here’s an example of a dynamic support agent that adjusts its behavior based on the user’s subscription tier and language preferences:
const supportAgent = new Agent({
  name: "Dynamic Support Agent",
 
  instructions: async ({ runtimeContext }) => {
    const userTier = runtimeContext.get("user-tier");
    const language = runtimeContext.get("language");
 
    return `You are a customer support agent for our SaaS platform.
    The current user is on the ${userTier} tier and prefers ${language} language.
    
    For ${userTier} tier users:
    ${userTier === "free" ? "- Provide basic support and documentation links" : ""}
    ${userTier === "pro" ? "- Offer detailed technical support and best practices" : ""}
    ${userTier === "enterprise" ? "- Provide priority support with custom solutions" : ""}
    
    Always respond in ${language} language.`;
  },
 
  model: ({ runtimeContext }) => {
    const userTier = runtimeContext.get("user-tier");
    return userTier === "enterprise"
      ? openai("gpt-4")
      : openai("gpt-3.5-turbo");
  },
 
  tools: ({ runtimeContext }) => {
    const userTier = runtimeContext.get("user-tier");
    const baseTools = [knowledgeBase, ticketSystem];
 
    if (userTier === "pro" || userTier === "enterprise") {
      baseTools.push(advancedAnalytics);
    }
 
    if (userTier === "enterprise") {
      baseTools.push(customIntegration);
    }
 
    return baseTools;
  },
});
In this example, the agent:
* Adjusts its instructions based on the user’s subscription tier (free, pro, or enterprise)
* Uses a more powerful model (GPT-4) for enterprise users
* Provides different sets of tools based on the user’s tier
* Responds in the user’s preferred language
This demonstrates how a single agent can handle different types of users and scenarios by leveraging runtime context, making it more flexible and maintainable than creating separate agents for each use case.
For a complete implementation example including API routes, middleware setup, and runtime context handling, see our Dynamic Agents Example.


Tools Overview
Tools are functions that agents can execute to perform specific tasks or access external information. They extend an agent’s capabilities beyond simple text generation, allowing interaction with APIs, databases, or other systems.
Each tool typically defines:
* Inputs: What information the tool needs to run (defined with an inputSchema, often using Zod).
* Outputs: The structure of the data the tool returns (defined with an outputSchema).
* Execution Logic: The code that performs the tool’s action.
* Description: Text that helps the agent understand what the tool does and when to use it.
Creating Tools
In Mastra, you create tools using the createTool function from the @mastra/core/tools package.
src/mastra/tools/weatherInfo.ts
import { createTool } from "@mastra/core/tools";
import { z } from "zod";
 
const getWeatherInfo = async (city: string) => {
  // Replace with an actual API call to a weather service
  console.log(`Fetching weather for ${city}...`);
  // Example data structure
  return { temperature: 20, conditions: "Sunny" };
};
 
export const weatherTool = createTool({
  id: "Get Weather Information",
  description: `Fetches the current weather information for a given city`,
  inputSchema: z.object({
    city: z.string().describe("City name"),
  }),
  outputSchema: z.object({
    temperature: z.number(),
    conditions: z.string(),
  }),
  execute: async ({ context: { city } }) => {
    console.log("Using tool to fetch weather information for", city);
    return await getWeatherInfo(city);
  },
});
This example defines a weatherTool with an input schema for the city, an output schema for the weather data, and an execute function that contains the tool’s logic.
When creating tools, keep tool descriptions simple and focused on what the tool does and when to use it, emphasizing its primary use case. Technical details belong in the parameter schemas, guiding the agent on how to use the tool correctly with descriptive names, clear descriptions, and explanations of default values.
MCP Overview
Model Context Protocol (MCP) 
is an open standard designed to let AI models discover and interact with external tools and resources. Think of it as a universal plugin system for AI agents, allowing them to use tools regardless of the language they were written in or where they are hosted.
Mastra uses MCP to connect agents to external tool servers.
Use third-party tools with an MCP Client
Mastra provides the MCPClient class to manage connections to one or more MCP servers and access their tools.
Installation
If you haven’t already, install the Mastra MCP package:
pnpm add @mastra/mcp@latest
Registering the MCPServer
Register your MCP server with Mastra to enable logging and access to configured tools and integrations:
src/mastra/index.ts
import { Mastra } from "@mastra/core";
import { myMcpServer } from "./mcpServers";
 
export const mastra = new Mastra({
  mcpServers: { myMcpServer },
});
Configuring MCPClient
You configure MCPClient with a map of servers you want to connect to. It supports connections via subprocess (Stdio) or HTTP (Streamable HTTP with SSE fallback).
import { MCPClient } from "@mastra/mcp";
 
const mcp = new MCPClient({
  servers: {
    // Stdio example
    sequential: {
      command: "npx",
      args: ["-y", "@modelcontextprotocol/server-sequential-thinking"],
    },
    // HTTP example
    weather: {
      url: new URL("http://localhost:8080/mcp"),
      requestInit: {
        headers: {
          Authorization: "Bearer your-token",
        },
      },
    },
  },
});
For detailed configuration options, see the MCPClient reference documentation.
Static vs Dynamic Tool Configurations
MCPClient offers two approaches to retrieving tools from connected servers, suitable for different application architectures:
Feature
	Static Configuration (await mcp.getTools())
	Dynamic Configuration (await mcp.getToolsets())
	Use Case
	Single-user, static config (e.g., CLI tool)
	Multi-user, dynamic config (e.g., SaaS app)
	Configuration
	Fixed at agent initialization
	Per-request, dynamic
	Credentials
	Shared across all uses
	Can vary per user/request
	Agent Setup
	Tools added in Agent constructor
	Tools passed in generate() or stream() options
	

Static Configuration (getTools()): Fetches all tools from all configured servers. Best when the tool configuration (like API keys) is static and shared across all users or requests. You typically call this once and pass the result to the tools property when defining your Agent. Reference: getTools()

 import { Agent } from "@mastra/core/agent";
// ... mcp client setup
 
const agent = new Agent({
  // ... other agent config
  tools: await mcp.getTools(),
* });

Dynamic Configuration (getToolsets()): Designed for scenarios where configuration might change per request or per user (e.g., different API keys for different tenants in a multi-user application). You pass the result of getToolsets() to the toolsets option in the agent’s generate() or stream() method. Reference: getToolsets()

 import { Agent } from "@mastra/core/agent";
// ... agent setup without tools initially
 
async function handleRequest(userPrompt: string, userApiKey: string) {
  const userMcp = new MCPClient({
    /* config with userApiKey */
  });
  const toolsets = await userMcp.getToolsets();
 
  const response = await agent.stream(userPrompt, {
    toolsets, // Pass dynamic toolsets
  });
  // ... handle response
  await userMcp.disconnect();
   * }

Connecting to an MCP registry
MCP servers can be discovered through registries. Here’s how to connect to some popular ones using MCPClient:
mcp.run 
provides pre-authenticated, managed MCP servers. Tools are grouped into Profiles, each with a unique, signed URL.
import { MCPClient } from "@mastra/mcp";
 
const mcp = new MCPClient({
  servers: {
    marketing: { // Example profile name
      url: new URL(process.env.MCP_RUN_SSE_URL!), // Get URL from mcp.run profile
    },
  },
});
Important: Treat the mcp.run SSE URL like a password. Store it securely, for example, in an environment variable.
.env
MCP_RUN_SSE_URL=https://www.mcp.run/api/mcp/sse?nonce=...
Share your tools with an MCP server
If you have created your own Mastra tools, you can expose them to any MCP-compatible client using Mastra’s MCPServer class.
Similarly, Mastra Agent and Workflow instances can also be exposed as tools via MCPServer. This allows other MCP clients to interact with your agents by “asking” them questions or run your workflows. Each agent provided in the MCPServer configuration will be converted into a tool named ask_<agentKey>, using the agent’s description property. Each workflow will be converted into a tool named run_<workflowKey>, using its inputSchema and description.
This allows others to use your tools, agents, and workflows without needing direct access to your codebase.
Using MCPServer
You initialize MCPServer with a name, version, and the Mastra tools, agents, and/or workflows you want to share.
import { MCPServer } from "@mastra/mcp";
import { Agent } from "@mastra/core/agent";
import { openai } from "@ai-sdk/openai";
import { weatherTool } from "./tools"; // Your Mastra tool
import { weatherAgent } from "./agents"; // Your Mastra Agent
import { dataWorkflow } from "./workflows"; // Your Mastra Workflow
 
const server = new MCPServer({
  name: "My Custom Server",
  version: "1.0.0",
  tools: { weatherTool }, // Provide your tool(s) here
  agents: { weatherAgent }, // Provide your agent(s) here
  workflows: { dataWorkflow }, // Provide your workflow(s) here
});
 
// Start the server (e.g., using stdio for a CLI tool)
// await server.startStdio();
 
// Or integrate with an HTTP server using startSSE()
// See MCPServer reference for details
For an agent to be exposed as a tool, it must have a non-empty description string. Similarly, for a workflow to be exposed, its description must also be a non-empty string. If the description is missing or empty for either, MCPServer will throw an error during initialization. Workflows will use their inputSchema for the tool’s input.
Tools with Structured Outputs
You can define an outputSchema for your tools to enforce a specific structure for the tool’s output. This is useful for ensuring that the tool returns data in a consistent and predictable format, which can then be validated by the client.
When a tool includes an outputSchema, its execute function must return an object. The value of the object must conform to the outputSchema. Mastra will automatically validate this output on both the server and client sides.
Here’s an example of a tool with an outputSchema:
src/tools/structured-tool.ts
import { createTool } from '@mastra/core';
import { z } from 'zod';
 
export const structuredTool = createTool({
  description: 'A test tool that returns structured data.',
  parameters: z.object({
    input: z.string().describe('Some input string.'),
  }),
  outputSchema: z.object({
    processedInput: z.string().describe('The processed input string.'),
    timestamp: z.string().describe('An ISO timestamp.'),
  }),
  execute: async ({ input }) => {
    // When outputSchema is defined, you must return an object
    return {
      processedInput: `processed: ${input}`,
      timestamp: new Date().toISOString(),
    };
  },
});
When this tool is called, the MCP client will receive both the structured data and a text representation of it.
Tool result


{
  "content": [
    {
      "type": "text",
      "text": "{\"processedInput\": \"hello\", \"timestamp\": \"2025-06-19T16:53:16.472Z\"}"
    }
  ],
  "structuredContent": {
    "processedInput": "processed: hello",
    "timestamp": "2025-06-19T16:53:16.472Z",
  }
}
Dynamic Tool Context
Mastra provides RuntimeContext, a system based on dependency injection, that allows you to pass dynamic, request-specific configuration to your tools during execution. This is useful when a tool’s behavior needs to change based on user identity, request headers, or other runtime factors, without altering the tool’s core code.
Note: RuntimeContext is primarily used for passing data into tool executions. It’s distinct from agent memory, which handles conversation history and state persistence across multiple calls.
Basic Usage
To use RuntimeContext, first define a type structure for your dynamic configuration. Then, create an instance of RuntimeContext typed with your definition and set the desired values. Finally, include the runtimeContext instance in the options object when calling agent.generate() or agent.stream().
import { RuntimeContext } from "@mastra/core/di";
// Assume 'agent' is an already defined Mastra Agent instance
 
// Define the context type
type WeatherRuntimeContext = {
  "temperature-scale": "celsius" | "fahrenheit";
};
 
// Instantiate RuntimeContext and set values
const runtimeContext = new RuntimeContext<WeatherRuntimeContext>();
runtimeContext.set("temperature-scale", "celsius");
 
// Pass to agent call
const response = await agent.generate("What's the weather like today?", {
  runtimeContext, // Pass the context here
});
 
console.log(response.text);
Accessing Context in Tools
Tools receive the runtimeContext as part of the second argument to their execute function. You can then use the .get() method to retrieve values.
src/mastra/tools/weather-tool.ts
import { createTool } from "@mastra/core/tools";
import { z } from "zod";
// Assume WeatherRuntimeContext is defined as above and accessible here
 
// Dummy fetch function
async function fetchWeather(
  location: string,
  options: { temperatureUnit: "celsius" | "fahrenheit" },
): Promise<any> {
  console.log(`Fetching weather for ${location} in ${options.temperatureUnit}`);
  // Replace with actual API call
  return { temperature: options.temperatureUnit === "celsius" ? 20 : 68 };
}
 
export const weatherTool = createTool({
  id: "getWeather",
  description: "Get the current weather for a location",
  inputSchema: z.object({
    location: z.string().describe("The location to get weather for"),
  }),
  // The tool's execute function receives runtimeContext
  execute: async ({ context, runtimeContext }) => {
    // Type-safe access to runtimeContext variables
    const temperatureUnit = runtimeContext.get("temperature-scale");
 
    // Use the context value in the tool logic
    const weather = await fetchWeather(context.location, {
      temperatureUnit,
    });
 
    return {
      result: `The temperature is ${weather.temperature}°${temperatureUnit === "celsius" ? "C" : "F"}`,
    };
  },
});
When the agent uses weatherTool, the temperature-scale value set in the runtimeContext during the agent.generate() call will be available inside the tool’s execute function.
Using with Server Middleware
In server environments (like Express or Next.js), you can use middleware to automatically populate RuntimeContext based on incoming request data, such as headers or user sessions.
Here’s an example using Mastra’s built-in server middleware support (which uses Hono internally) to set the temperature scale based on the Cloudflare CF-IPCountry header:
src/mastra/index.ts
import { Mastra } from "@mastra/core";
import { RuntimeContext } from "@mastra/core/di";
import { weatherAgent } from "./agents/weather"; // Assume agent is defined elsewhere
 
// Define RuntimeContext type
type WeatherRuntimeContext = {
  "temperature-scale": "celsius" | "fahrenheit";
};
 
export const mastra = new Mastra({
  agents: {
    weather: weatherAgent,
  },
  server: {
    middleware: [
      async (c, next) => {
        // Get the RuntimeContext instance
        const runtimeContext =
          c.get<RuntimeContext<WeatherRuntimeContext>>("runtimeContext");
 
        // Get country code from request header
        const country = c.req.header("CF-IPCountry");
 
        // Set temperature scale based on country
        runtimeContext.set(
          "temperature-scale",
          country === "US" ? "fahrenheit" : "celsius",
        );
 
        // Continue request processing
        await next();
      },
    ],
  },
});
With this middleware in place, any agent call handled by this Mastra server instance will automatically have the temperature-scale set in its RuntimeContext based on the user’s inferred country, and tools like weatherTool will use it accordingly.
Advanced Tool Usage
This page covers more advanced techniques and features related to using tools in Mastra.
Abort Signals
When you initiate an agent interaction using generate() or stream(), you can provide an AbortSignal. Mastra automatically forwards this signal to any tool executions that occur during that interaction.
This allows you to cancel long-running operations within your tools, such as network requests or intensive computations, if the parent agent call is aborted.
You access the abortSignal in the second parameter of the tool’s execute function.
import { createTool } from "@mastra/core/tools";
import { z } from "zod";
 
export const longRunningTool = createTool({
  id: "long-computation",
  description: "Performs a potentially long computation",
  inputSchema: z.object({ /* ... */ }),
  execute: async ({ context }, { abortSignal }) => {
    // Example: Forwarding signal to fetch
    const response = await fetch("https://api.example.com/data", {
      signal: abortSignal, // Pass the signal here
    });
 
    if (abortSignal?.aborted) {
      console.log("Tool execution aborted.");
      throw new Error("Aborted");
    }
 
    // Example: Checking signal during a loop
    for (let i = 0; i < 1000000; i++) {
      if (abortSignal?.aborted) {
        console.log("Tool execution aborted during loop.");
        throw new Error("Aborted");
      }
      // ... perform computation step ...
    }
 
    const data = await response.json();
    return { result: data };
  },\n});
To use this, provide an AbortController’s signal when calling the agent:
import { Agent } from "@mastra/core/agent";
// Assume 'agent' is an Agent instance with longRunningTool configured
 
const controller = new AbortController();
 
// Start the agent call
const promise = agent.generate("Perform the long computation.", {
  abortSignal: controller.signal,
});
 
// Sometime later, if needed:
// controller.abort();
 
try {
  const result = await promise;
  console.log(result.text);
} catch (error) {
  if (error.name === "AbortError") {
    console.log("Agent generation was aborted.");
  } else {
    console.error("An error occurred:", error);
  }
}
AI SDK Tool Format
Mastra maintains compatibility with the tool format used by the Vercel AI SDK (ai package). You can define tools using the tool function from the ai package and use them directly within your Mastra agents alongside tools created with Mastra’s createTool.
First, ensure you have the ai package installed:
pnpm add ai
Here’s an example of a tool defined using the Vercel AI SDK format:
src/mastra/tools/vercelWeatherTool.ts
import { tool } from "ai";
import { z } from "zod";
 
export const vercelWeatherTool = tool({
  description: "Fetches current weather using Vercel AI SDK format",
  parameters: z.object({
    city: z.string().describe("The city to get weather for"),
  }),
  execute: async ({ city }) => {
    console.log(`Fetching weather for ${city} (Vercel format tool)`);
    // Replace with actual API call
    const data = await fetch(`https://api.example.com/weather?city=${city}`);
    return data.json();
  },
});
You can then add this tool to your Mastra agent just like any other tool:
src/mastra/agents/mixedToolsAgent.ts
import { Agent } from "@mastra/core/agent";
import { openai } from "@ai-sdk/openai";
import { vercelWeatherTool } from "../tools/vercelWeatherTool"; // Vercel AI SDK tool
import { mastraTool } from "../tools/mastraTool"; // Mastra createTool tool
 
export const mixedToolsAgent = new Agent({
  name: "Mixed Tools Agent",
  instructions: "You can use tools defined in different formats.",
  model: openai("gpt-4o-mini"),
  tools: {
    weatherVercel: vercelWeatherTool,
    someMastraTool: mastraTool,
  },
});
Mastra supports both tool formats, allowing you to mix and match as needed.


Workflows overview
Workflows let you define and orchestrate complex sequences of tasks as typed steps connected by data flows. Each step has clearly defined inputs and outputs validated by Zod schemas.
A workflow manages execution order, dependencies, branching, parallelism, and error handling — enabling you to build robust, reusable processes. Steps can be nested or cloned to compose larger workflows.
 Workflows overview 

You create workflows by:
      * Defining steps with createStep, specifying input/output schemas and business logic.
      * Composing steps with createWorkflow to define the execution flow.
      * Running workflows to execute the entire sequence, with built-in support for suspension, resumption, and streaming results.
This structure provides full type safety and runtime validation, ensuring data integrity across the entire workflow.
Getting started
To use workflows, first import the necessary functions from the workflows module:
src/mastra/workflows/test-workflow.ts
import { createWorkflow, createStep } from "@mastra/core/workflows";
import { z } from "zod";
Create step
Steps are the building blocks of workflows. Create a step using createStep:
src/mastra/workflows/test-workflow.ts
const step1 = createStep({...});
See createStep for more information.
Create workflow
Create a workflow using createWorkflow and complete it with .commit().
src/mastra/workflows/test-workflow.ts
import { createWorkflow, createStep } from "@mastra/core/workflows";
import { z } from "zod";
 
const step1 = createStep({...});
 
export const testWorkflow = createWorkflow({
  id: "test-workflow",
  description: 'Test workflow',
  inputSchema: z.object({
    input: z.string()
  }),
  outputSchema: z.object({
    output: z.string()
  })
})
  .then(step1)
  .commit();
See workflow for more information.
Composing steps
Workflow steps can be composed and executed sequentially using .then().
src/mastra/workflows/test-workflow.ts
import { createWorkflow, createStep } from "@mastra/core/workflows";
import { z } from "zod";
 
const step1 = createStep({...});
const step2 = createStep({...});
 
export const testWorkflow = createWorkflow({
  id: "test-workflow",
  description: 'Test workflow',
  inputSchema: z.object({
    input: z.string()
  }),
  outputSchema: z.object({
    output: z.string()
  })
})
  .then(step1)
  .then(step2)
  .commit();
Steps can be composed using a number of different methods. See Control Flow for more information.
Cloning steps
Workflow steps can be cloned using cloneStep(), and used with any workflow method.
src/mastra/workflows/test-workflow.ts
import { createWorkflow, createStep, cloneStep } from "@mastra/core/workflows";
import { z } from "zod";
 
const step1 = createStep({...});
const clonedStep = cloneStep(step1, { id: "cloned-step" });
const step2 = createStep({...});
 
export const testWorkflow = createWorkflow({
  id: "test-workflow",
  description: 'Test workflow',
  inputSchema: z.object({
    input: z.string()
  }),
  outputSchema: z.object({
    output: z.string()
  })
})
  .then(step1)
  .then(clonedStep)
  .then(step2)
  .commit();
Register workflow
Register a workflow using workflows in the main Mastra instance:
src/mastra/index.ts
import { Mastra } from "@mastra/core/mastra";
import { PinoLogger } from "@mastra/loggers";
import { LibSQLStore } from "@mastra/libsql";
 
import { testWorkflow } from "./workflows/test-workflow";
 
export const mastra = new Mastra({
  workflows: { testWorkflow },
  storage: new LibSQLStore({
    // stores telemetry, evals, ... into memory storage, if it needs to persist, change to file:../mastra.db
    url: ":memory:"
  }),
  logger: new PinoLogger({
    name: "Mastra",
    level: "info"
  })
});
Run workflow
There are two ways to run and test workflows.
Mastra Playground
With the Mastra Dev Server running you can run the workflow from the Mastra Playground by visiting http://localhost:4111/workflows 
in your browser.
Command line
Create a run instance of any Mastra workflow using createRunAsync and start:
src/test-workflow.ts
import { mastra } from "./mastra";
 
const run = await mastra.getWorkflow("testWorkflow").createRunAsync();
 
const result = await run.start({
  inputData: {
    city: "London"
  }
});
 
console.log(JSON.stringify(result, null, 2));
see createRunAsync and start for more information.
To trigger this workflow, run the following:
npx tsx src/test-workflow.ts
Run workflow results
The result of running a workflow using either start() or resume() will look like one of the following, depending on the outcome.
Status success
{
  "status": "success",
  "steps": {
    // ...
    "step-1": {
      // ...
      "status": "success",
    }
  },
  "result": {
    "output": "London + step-1"
  }
}
      * status: Shows the final state of the workflow execution, either: success, suspended, or error
      * steps: Lists each step in the workflow, including inputs and outputs
      * status: Shows the outcome of each individual step
      * result: Includes the final output of the workflow, typed according to the outputSchema
Status suspended
{
  "status": "suspended",
  "steps": {
    // ...
    "step-1": {
      // ...
      "status": "suspended",
    }
  },
  "suspended": [
    [
      "step-1"
    ]
  ]
}
      * suspended: An optional array listing any steps currently awaiting input before continuing
Status failed
{
  "status": "failed",
  "steps": {
    // ...
    "step-1": {
      // ...
      "status": "failed",
      "error": "Test error",
    }
  },
  "error": "Test error"
}
      * error: An optional field that includes the error message if the workflow fails
Stream workflow
Similar to the run method shown above, workflows can also be streamed:
src/test-workflow.ts
import { mastra } from "./mastra";
 
const run = await mastra.getWorkflow("testWorkflow").createRunAsync();
 
const result = await run.stream({
  inputData: {
    city: "London"
  }
});
 
for await (const chunk of result.stream) {
  console.log(chunk);
}
See stream and messages for more information.
Watch Workflow
A workflow can also be watched, allowing you to inspect each event that is emitted.
src/test-workflow.ts
import { mastra } from "./mastra";
 
const run = await mastra.getWorkflow("testWorkflow").createRunAsync();
 
run.watch((event) => {
  console.log(event);
});
 
const result = await run.start({
  inputData: {
    city: "London"
  }
});
See watch for more information.
Control Flow
When you build a workflow, you typically break down operations into smaller tasks that can be linked and reused. Steps provide a structured way to manage these tasks by defining inputs, outputs, and execution logic.
      * If the schemas match, the outputSchema from each step is automatically passed to the inputSchema of the next step.
      * If the schemas don’t match, use Input data mapping to transform the outputSchema into the expected inputSchema.
Chaining steps with .then()
Chain steps to execute sequentially using .then():
 Chaining steps with .then() 

src/mastra/workflows/test-workflow.ts
import { createWorkflow, createStep } from "@mastra/core/workflows";
import { z } from "zod";
 
const step1 = createStep({...});
const step2 = createStep({...});
 
export const testWorkflow = createWorkflow({...})
  .then(step1)
  .then(step2)
  .commit();
This does what you’d expect: it executes step1, then it executes step2.
Simultaneous steps with .parallel()
Execute steps simultaneously using .parallel():
 Concurrent steps with .parallel() 

src/mastra/workflows/test-workflow.ts
import { createWorkflow, createStep } from "@mastra/core/workflows";
import { z } from "zod";
 
const step1 = createStep({...});
const step2 = createStep({...});
const step3 = createStep({...});
 
export const testWorkflow = createWorkflow({...})
  .parallel([step1, step2])
  .then(step3)
  .commit();
This executes step1 and step2 concurrently, then continues to step3 after both complete.
See Parallel Execution with Steps for more information.
Conditional logic with .branch()
Execute steps conditionally using .branch():
 Conditional branching with .branch() 

src/mastra/workflows/test-workflow.ts
import { createWorkflow, createStep } from "@mastra/core/workflows";
import { z } from "zod";
 
const lessThanStep = createStep({...});
const greaterThanStep = createStep({...});
 
export const testWorkflow = createWorkflow({...})
  .branch([
    [async ({ inputData: { value } }) => (value < 9), lessThanStep],
    [async ({ inputData: { value } }) => (value >= 9), greaterThanStep]
  ])
  .commit();
Branch conditions are evaluated sequentially, but steps with matching conditions are executed in parallel.
See Workflow with Conditional Branching for more information.
Looping steps
Workflows support two types of loops. When looping a step, or any step-compatible construct like a nested workflow, the initial inputData is sourced from the output of the previous step.
To ensure compatibility, the loop’s initial input must either match the shape of the previous step’s output, or be explicitly transformed using the map function.
      * Match the shape of the previous step’s output, or
      * Be explicitly transformed using the map function.
Repeating with .dowhile()
Executes step repeatedly while a condition is true.
 Repeating with .dowhile() 

src/mastra/workflows/test-workflow.ts
import { createWorkflow, createStep } from "@mastra/core/workflows";
import { z } from "zod";
 
const counterStep = createStep({...});
 
export const testWorkflow = createWorkflow({...})
  .dowhile(counterStep, async ({ inputData: { number } }) => number < 10)
  .commit();
Repeating with .dountil()
Executes step repeatedly until a condition becomes true.
 Repeating with .dountil() 

src/mastra/workflows/test-workflow.ts
import { createWorkflow, createStep } from "@mastra/core/workflows";
import { z } from "zod";
 
const counterStep = createStep({...});
 
export const testWorkflow = createWorkflow({...})
  .dountil(counterStep, async ({ inputData: { number } }) => number > 10)
  .commit();
Repeating with .foreach()
Sequentially executes the same step for each item from the inputSchema.
 Repeating with .foreach() 

src/mastra/workflows/test-workflow.ts
import { createWorkflow, createStep } from "@mastra/core/workflows";
import { z } from "zod";
 
const mapStep = createStep({...});
 
export const testWorkflow = createWorkflow({...})
  .foreach(mapStep)
  .commit();
Setting concurrency limits
Use concurrency to execute steps in parallel with a limit on the number of concurrent executions.
src/mastra/workflows/test-workflow.ts
import { createWorkflow, createStep } from "@mastra/core/workflows";
import { z } from "zod";
 
const mapStep = createStep({...})
 
export const testWorkflow = createWorkflow({...})
  .foreach(mapStep, { concurrency: 2 })
  .commit();
Using a nested workflow
Use a nested workflow as a step by passing it to .then(). This runs each of its steps in sequence as part of the parent workflow.
src/mastra/workflows/test-workflow.ts
import { createWorkflow, createStep } from "@mastra/core/workflows";
import { z } from "zod";
 
export const nestedWorkflow = createWorkflow({...})
 
export const testWorkflow = createWorkflow({...})
  .then(nestedWorkflow)
  .commit();
Cloning a workflow
Use cloneWorkflow to duplicate an existing workflow. This lets you reuse its structure while overriding parameters like id.
src/mastra/workflows/test-workflow.ts
import { createWorkflow, createStep, cloneWorkflow } from "@mastra/core/workflows";
import { z } from "zod";
 
const step1 = createStep({...});
const parentWorkflow = createWorkflow({...})
const clonedWorkflow = cloneWorkflow(parentWorkflow, { id: "cloned-workflow" });
 
export const testWorkflow = createWorkflow({...})
  .then(step1)
  .then(clonedWorkflow)
  .commit();
Exiting early with bail()
Use bail() in a step to exit early with a successful result. This returns the provided payload as the step output and ends workflow execution.
src/mastra/workflows/test-workflow.ts
import { createWorkflow, createStep } from "@mastra/core/workflows";
import { z } from "zod";
 
const step1 = createStep({
  id: 'step1',
  execute: async ({ bail }) => {
    return bail({ result: 'bailed' });
  },
  inputSchema: z.object({ value: z.string() }),
  outputSchema: z.object({ result: z.string() }),
});
 
export const testWorkflow = createWorkflow({...})
  .then(step1)
  .commit();
Exiting early with Error()
Use throw new Error() in a step to exit with an error.
src/mastra/workflows/test-workflow.ts
import { createWorkflow, createStep } from "@mastra/core/workflows";
import { z } from "zod";
 
const step1 = createStep({
  id: 'step1',
  execute: async () => {
    throw new Error('bailed');
  },
  inputSchema: z.object({ value: z.string() }),
  outputSchema: z.object({ result: z.string() }),
});
 
export const testWorkflow = createWorkflow({...})
  .then(step1)
  .commit();
This throws an error from the step and stops workflow execution, returning the error as the result.
Example Run Instance
The following example demonstrates how to start a run with multiple inputs. Each input will pass through the mapStep sequentially.
src/test-workflow.ts
import { mastra } from "./mastra";
 
const run = await mastra.getWorkflow("testWorkflow").createRunAsync();
 
const result = await run.start({
  inputData: [{ number: 10 }, { number: 100 }, { number: 200 }]
});
To execute this run from your terminal:
npx tsx src/test-workflow.ts


Sleep & Events
Mastra lets you pause workflow execution when waiting for external input or timing conditions. This can be useful for things like polling, delayed retries, or waiting on user actions.
You can pause execution using:
      * sleep(): Pause for a set number of milliseconds
      * sleepUntil(): Pause until a specific timestamp
      * waitForEvent(): Pause until an external event is received
      * sendEvent(): Send an event to resume a waiting workflow
When using any of these methods, the workflow status is set to waiting until execution resumes.
Pausing with .sleep()
The sleep() method pauses execution between steps for a specified number of milliseconds.
src/mastra/workflows/test-workflow.ts
import { createWorkflow, createStep } from "@mastra/core/workflows";
import { z } from "zod";
 
const step1 = createStep({...});
const step2 = createStep({...});
 
export const testWorkflow = createWorkflow({...})
  .then(step1)
  .sleep(1000)
  .then(step2)
  .commit();
Pausing with .sleep(callback)
The sleep() method also accepts a callback that returns the number of milliseconds to pause. The callback receives inputData, allowing the delay to be computed dynamically.
src/mastra/workflows/test-workflow.ts
import { createWorkflow, createStep } from "@mastra/core/workflows";
import { z } from "zod";
 
const step1 = createStep({...});
const step2 = createStep({...});
 
export const testWorkflow = createWorkflow({...})
  .then(step1)
  .sleep(async ({ inputData }) => {
    const { delayInMs }  = inputData
    return delayInMs;
  })
  .then(step2)
  .commit();
Pausing with .sleepUntil()
The sleepUntil() method pauses execution between steps until a specified date.
src/mastra/workflows/test-workflow.ts
import { createWorkflow, createStep } from "@mastra/core/workflows";
import { z } from "zod";
 
const step1 = createStep({...});
const step2 = createStep({...});
 
export const testWorkflow = createWorkflow({...})
  .then(step1)
  .sleepUntil(new Date(Date.now() + 5000))
  .then(step2)
  .commit();
Pausing with .sleepUntil(callback)
The sleepUntil() method also accepts a callback that returns a Date object. The callback receives inputData, allowing the target time to be computed dynamically.
src/mastra/workflows/test-workflow.ts
import { createWorkflow, createStep } from "@mastra/core/workflows";
import { z } from "zod";
 
const step1 = createStep({...});
const step2 = createStep({...});
 
export const testWorkflow = createWorkflow({...})
  .then(step1)
  .sleepUntil(async ({ inputData }) => {
    const { delayInMs }  = inputData
    return new Date(Date.now() + delayInMs);
  })
  .then(step2)
  .commit();
Date.now() is evaluated when the workflow starts, not at the moment the sleepUntil() method is called.
Pausing with .waitForEvent()
The waitForEvent() method pauses execution until a specific event is received. Use run.sendEvent() to send the event. You must provide both the event name and the step to resume.
 Pausing with .waitForEvent() 

src/mastra/workflows/test-workflow.ts
import { createWorkflow, createStep } from "@mastra/core/workflows";
import { z } from "zod";
 
const step1 = createStep({...});
const step2 = createStep({...});
const step3 = createStep({...});
 
export const testWorkflow = createWorkflow({...})
  .then(step1)
  .waitForEvent("my-event-name", step2)
  .then(step3)
  .commit();
Sending an event with .sendEvent()
The .sendEvent() method sends an event to the workflow. It accepts the event name and optional event data, which can be any JSON-serializable value.
src/test-workflow.ts
import { mastra } from "./mastra";
 
const run = await mastra.getWorkflow("testWorkflow").createRunAsync();
 
const result = run.start({
  inputData: {
    value: "hello"
  }
});
 
setTimeout(() => {
  run.sendEvent("my-event-name", { value: "from event" });
}, 3000);
 
console.log(JSON.stringify(await result, null, 2));
In this example, avoid using await run.start() directly, it would block sending the event before the workflow reaches its waiting state.
Input Data Mapping
Input data mapping allows explicit mapping of values for the inputs of the next step. These values can come from a number of sources:
      * The outputs of a previous step
      * The runtime context
      * A constant value
      * The initial input of the workflow
Mapping with .map()
In this example the output from step1 is transformed to match the inputSchema required for the step2. The value from step1 is available using the inputData parameter of the .map function.
 Mapping with .map() 

src/mastra/workflows/test-workflow.ts
const step1 = createStep({...});
const step2 = createStep({...});
 
export const testWorkflow = createWorkflow({...})
  .then(step1)
  .map(async ({ inputData }) => {
    const { value } = inputData;
    return {
      output: `new ${value}`
    };
  })
  .then(step2)
  .commit();
Using inputData
Use inputData to access the full output of the previous step:
src/mastra/workflows/test-workflow.ts
 .then(step1)
  .map(({ inputData }) => {
    console.log(inputData);
  })
Using getStepResult()
Use getStepResult to access the full output of a specific step by referencing the step’s instance:
src/mastra/workflows/test-workflow.ts
 .then(step1)
  .map(async ({ getStepResult }) => {
    console.log(getStepResult(step1));
  })
Using getInitData()
Use getInitData to access the initial input data provided to the workflow:
src/mastra/workflows/test-workflow.ts
 .then(step1)
  .map(async ({ getInitData }) => {
      console.log(getInitData());
  })
Using mapVariable()
To use mapVariable import the necessary function from the workflows module:
src/mastra/workflows/test-workflow.ts
import { mapVariable } from "@mastra/core/workflows";
Renaming step with mapVariable()
You can rename step outputs using the object syntax in .map(). In the example below, the value output from step1 is renamed to details:
src/mastra/workflows/test-workflow.ts
 .then(step1)
  .map({
    details: mapVariable({
      step: step,
      path: "value"
    })
  })
Renaming workflows with mapVariable()
You can rename workflow outputs by using referential composition. This involves passing the workflow instance as the initData.
src/mastra/workflows/test-workflow.ts
export const testWorkflow = createWorkflow({...});
 
testWorkflow
  .then(step1)
  .map({
    details: mapVariable({
      initData: testWorkflow,
      path: "value"
    })
  })




Agents and Tools
Workflow steps are composable and typically run logic directly within the execute function. However, there are cases where calling an agent or tool is more appropriate. This pattern is especially useful when:
      * Generating natural language responses from user input using an LLM.
      * Abstracting complex or reusable logic into a dedicated tool.
      * Interacting with third-party APIs in a structured or reusable way.
Workflows can use Mastra agents or tools directly as steps, for example: createStep(testAgent) or createStep(testTool).
Using agents in workflows
To include an agent in a workflow, define it in the usual way, then either add it directly to the workflow using createStep(testAgent) or, invoke it from within a step’s execute function using .generate().
Example agent
This agent uses OpenAI to generate a fact about a city, country, and timezone.
src/mastra/agents/test-agent.ts
import { openai } from "@ai-sdk/openai";
import { Agent } from "@mastra/core/agent";
 
export const testAgent = new Agent({
  name: "test-agent",
  description: "Create facts for a country based on the city",
  instructions: `Return an interesting fact about the country based on the city provided`,
  model: openai("gpt-4o")
});
Adding an agent as a step
In this example, step1 uses the testAgent to generate an interesting fact about the country based on a given city.
The .map method transforms the workflow input into a prompt string compatible with the testAgent.
The step is composed into the workflow using .then(), allowing it to receive the mapped input and return the agent’s structured output. The workflow is finalized with .commit().
 Agent as step 

src/mastra/workflows/test-workflow.ts
import { testAgent } from "../agents/test-agent";
 
const step1 = createStep(testAgent);
 
export const testWorkflow = createWorkflow({
  id: "test-workflow",
  description: 'Test workflow',
  inputSchema: z.object({
    input: z.string()
  }),
  outputSchema: z.object({
    output: z.string()
  })
})
  .map(({ inputData }) => {
    const { input } = inputData;
    return {
      prompt: `Provide facts about the city: ${input}`
    };
  })
  .then(step1)
  .commit();
Calling an agent with .generate()
In this example, the step1 builds a prompt using the provided input and passes it to the testAgent, which returns a plain-text response containing facts about the city and its country.
The step is added to the workflow using the sequential .then() method, allowing it to receive input from the workflow and return structured output. The workflow is finalized with .commit().
src/mastra/workflows/test-workflow.ts
import { testAgent } from "../agents/test-agent";
 
const step1 = createStep({
  id: "step-1",
  description: "Create facts for a country based on the city",
  inputSchema: z.object({
    input: z.string()
  }),
  outputSchema: z.object({
    output: z.string()
  }),
 
  execute: async ({ inputData }) => {
    const { input } = inputData;
 
    const  prompt = `Provide facts about the city: ${input}`
 
    const { text } = await testAgent.generate([
      { role: "user", content: prompt }
    ]);
 
    return {
      output: text
    };
  }
});
 
export const testWorkflow = createWorkflow({...})
  .then(step1)
  .commit();
Using tools in workflows
To use a tool within a workflow, define it in the usual way, then either add it directly to the workflow using createStep(testTool) or, invoke it from within a step’s execute function using .execute().
Example tool
The example below uses the Open Meteo API to retrieve geolocation details for a city, returning its name, country, and timezone.
src/mastra/tools/test-tool.ts
import { createTool } from "@mastra/core";
import { z } from "zod";
 
export const testTool = createTool({
  id: "test-tool",
  description: "Gets country for a city",
  inputSchema: z.object({
    input: z.string()
  }),
  outputSchema: z.object({
    country_name: z.string()
  }),
  execute: async ({ context }) => {
    const { input } = context;
    const geocodingResponse = await fetch(`https://geocoding-api.open-meteo.com/v1/search?name=${input}`);
    const geocodingData = await geocodingResponse.json();
 
    const { country } = geocodingData.results[0];
 
    return {
      country_name: country
    };
  }
});
Adding a tool as a step
In this example, step1 uses the testTool, which performs a geocoding lookup using the provided city and returns the resolved country.
The step is added to the workflow using the sequential .then() method, allowing it to receive input from the workflow and return structured output. The workflow is finalized with .commit().
 Tool as step 

src/mastra/workflows/test-workflow.ts
import { testTool } from "../tools/test-tool";
 
const step1 = createStep(testTool);
 
export const testWorkflow = createWorkflow({...})
  .then(step1)
  .commit();
Calling a tool with .execute()
In this example, step1 directly invokes testTool using its .execute() method. The tool performs a geocoding lookup with the provided city and returns the corresponding country.
The result is returned as structured output from the step. The step is composed into the workflow using .then(), enabling it to process workflow input and produce typed output. The workflow is finalized with .commit()
src/mastra/workflows/test-workflow.ts
import { RuntimeContext } from "@mastra/core/di";
 
import { testTool } from "../tools/test-tool";
 
const runtimeContext = new RuntimeContext();
 
const step1 = createStep({
  id: "step-1",
  description: "Gets country for a city",
  inputSchema: z.object({
    input: z.string()
  }),
  outputSchema: z.object({
    output: z.string()
  }),
 
  execute: async ({ inputData }) => {
    const { input } = inputData;
 
    const { country_name } = await testTool.execute({
      context: { input },
      runtimeContext
    });
 
    return {
      output: country_name
    };
  }
});
 
export const testWorkflow = createWorkflow({...})
  .then(step1)
  .commit();
Using workflows as tools
In this example the cityStringWorkflow workflow has been added to the main Mastra instance.
src/mastra/index.ts
import { Mastra } from "@mastra/core/mastra";
 
import { testWorkflow, cityStringWorkflow } from "./workflows/test-workflow";
 
export const mastra = new Mastra({
  ...
  workflows: { testWorkflow, cityStringWorkflow },
});
Once a workflow has been registered it can be referenced using getWorkflow from withing a tool.
src/mastra/tools/test-tool.ts
export const cityCoordinatesTool = createTool({
  id: "city-tool",
  description: "Convert city details",
  inputSchema: z.object({
    city: z.string()
  }),
  outputSchema: z.object({
    outcome: z.string()
  }),
  execute: async ({ context, mastra }) => {
    const { city } = context;
    const geocodingResponse = await fetch(`https://geocoding-api.open-meteo.com/v1/search?name=${city}`);
    const geocodingData = await geocodingResponse.json();
 
    const { name, country, timezone } = geocodingData.results[0];
 
    const workflow = mastra?.getWorkflow("cityStringWorkflow");
 
    const run = await workflow?.createRunAsync();
 
    const { result } = await run?.start({
      inputData: {
        city_name: name,
        country_name: country,
        country_timezone: timezone
      }
    });
 
    return {
      outcome: result.outcome
    };
  }
});
Exposing workflows with MCPServer
You can convert your workflows into tools by passing them into an instance of a Mastra MCPServer. This allows any MCP-compatible client to access your workflow.
The workflow description becomes the tool description and the input schema becomes the tool’s input schema.
When you provide workflows to the server, each workflow is automatically exposed as a callable tool for example:
      * run_testWorkflow.
src/test-mcp-server.ts
import { MCPServer } from "@mastra/mcp";
 
import { testAgent } from "./mastra/agents/test-agent";
import { testTool } from "./mastra/tools/test-tool";
import { testWorkflow } from "./mastra/workflows/test-workflow";
 
async function startServer() {
  const server = new MCPServer({
    name: "test-mcp-server",
    version: "1.0.0",
    workflows: {
      testWorkflow
    }
  });
 
  await server.startStdio();
  console.log("MCPServer started on stdio");
}
 
startServer().catch(console.error);
To verify that your workflow is available on the server, you can connect with an MCPClient.
src/test-mcp-client.ts
import { MCPClient } from "@mastra/mcp";
 
async function main() {
  const mcp = new MCPClient({
    servers: {
      local: {
        command: "npx",
        args: ["tsx", "src/test-mcp-server.ts"]
      }
    }
  });
 
  const tools = await mcp.getTools();
  console.log(tools);
}
 
main().catch(console.error);
Run the client script to see your workflow tool.
npx tsx src/test-mcp-client.ts


Memory overview

Memory is how agents manage the context that’s available to them, it’s a condensation of all chat messages into their context window.
The Context Window

The context window is the total information visible to the language model at any given time.

In Mastra, context is broken up into three parts: system instructions and information about the user (working memory), recent messages (message history), and older messages that are relevant to the user’s query (semantic recall).

Working memory can persist at different scopes - either per conversation thread (default) or across all threads for the same user (resource-scoped), enabling persistent user profiles that remember context across conversations.

In addition, we provide memory processors to trim context or remove information if the context is too long.
Quick Start

The fastest way to see memory in action is using the built-in development playground.

If you haven’t already, create a new Mastra project following the main Getting Started guide.

1. Install the memory package:

pnpm add @mastra/memory@latest

2. Create an agent and attach a Memory instance:
src/mastra/agents/index.ts

import { Agent } from "@mastra/core/agent"; import { Memory } from "@mastra/memory"; import { openai } from "@ai-sdk/openai"; import { LibSQLStore } from "@mastra/libsql";  // Initialize memory with LibSQLStore for persistence const memory = new Memory({   storage: new LibSQLStore({     url: "file:../mastra.db", // Or your database URL   }), });  export const myMemoryAgent = new Agent({   name: "MemoryAgent",   instructions: "...",   model: openai("gpt-4o"),   memory, });

3. Start the Development Server:

pnpm run dev

4. Open the playground (http://localhost:4111 

) and select your MemoryAgent:

Send a few messages and notice that it remembers information across turns:

➡️ You: My favorite color is blue. ⬅️ Agent: Got it! I'll remember that your favorite color is blue. ➡️ You: What is my favorite color? ⬅️ Agent: Your favorite color is blue.

Memory Threads

Mastra organizes memory into threads, which are records that identify specific conversation histories, using two identifiers:

    threadId: A specific conversation id (e.g., support_123).
    resourceId: The user or entity id that owns each thread (e.g., user_123, org_456).

The resourceId is particularly important for resource-scoped working memory, which allows memory to persist across all conversation threads for the same user.

const response = await myMemoryAgent.stream("Hello, my name is Alice.", {   resourceId: "user_alice",   threadId: "conversation_123", });

Important: without these ID’s your agent will not use memory, even if memory is properly configured. The playground handles this for you, but you need to add ID’s yourself when using memory in your application.
Thread Title Generation

Mastra can automatically generate meaningful titles for conversation threads based on the user’s first message. This helps organize and identify conversations in your application UI.

const memory = new Memory({   options: {     threads: {       generateTitle: true, // Enable automatic title generation     },   }, });

By default, title generation uses the same model and default instructions as your agent. For customization or cost optimization, you can specify a different model or provide custom instructions specifically for title generation:

const memory = new Memory({   options: {     threads: {       generateTitle: {         model: openai("gpt-4.1-nano"), // Use cheaper model for titles         instructions: "Generate a concise title for this conversation based on the first user message.",       },     },   }, });

Title generation happens asynchronously after the agent responds, so it doesn’t impact response time. See the full configuration reference for more details and examples.
Conversation History

By default, the Memory instance includes the last 10 messages from the current Memory thread in each new request. This provides the agent with immediate conversational context.

const memory = new Memory({   options: {     lastMessages: 10,   }, });

Important: Only send the newest user message in each agent call. Mastra handles retrieving and injecting the necessary history. Sending the full history yourself will cause duplication. See the AI SDK Memory Example for how to handle this with when using the useChat frontend hooks.
Storage Configuration

Conversation history relies on a storage adapter to store messages. By default it uses the same storage provided to the main Mastra instance 

If neither the Memory instance nor the Mastra object specify a storage provider, Mastra will not persist memory data across application restarts or deployments. For any deployment beyond local testing you should provide your own storage configuration either on Mastra or directly within new Memory().

When storage is given on the Mastra instance it will automatically be used by every Memory attached to agents. In that case you do not need to pass storage to new Memory() unless you want a per-agent override.

import { Memory } from "@mastra/memory"; import { Agent } from "@mastra/core/agent"; import { LibSQLStore } from "@mastra/libsql";  const agent = new Agent({   memory: new Memory({     storage: new LibSQLStore({       url: "file:./local.db",     }),   }), });

Storage code Examples:

    LibSQL
    Postgres
    Upstash

Viewing Retrieved Messages

If tracing is enabled in your Mastra deployment and memory is configured either with lastMessages and/or semanticRecall, the agent’s trace output will show all messages retrieved for context—including both recent conversation history and messages recalled via semantic recall.

This is helpful for debugging, understanding agent decisions, and verifying that the agent is retrieving the right information for each request.

For more details on enabling and configuring tracing, see Tracing.
Next Steps

Now that you understand the core concepts, continue to semantic recall to learn how to add RAG memory to your Mastra agents.

Alternatively you can visit the configuration reference for available options, or browse usage examples.

Semantic Recall

If you ask your friend what they did last weekend, they will search in their memory for events associated with “last weekend” and then tell you what they did. That’s sort of like how semantic recall works in Mastra.
How Semantic Recall Works

Semantic recall is RAG-based search that helps agents maintain context across longer interactions when messages are no longer within recent conversation history.

It uses vector embeddings of messages for similarity search, integrates with various vector stores, and has configurable context windows around retrieved messages.

Diagram showing Mastra Memory semantic recall

When it’s enabled, new messages are used to query a vector DB for semantically similar messages.

After getting a response from the LLM, all new messages (user, assistant, and tool calls/results) are inserted into the vector DB to be recalled in later interactions.
Quick Start

Semantic recall is enabled by default, so if you give your agent memory it will be included:

import { Agent } from "@mastra/core/agent"; import { Memory } from "@mastra/memory"; import { openai } from "@ai-sdk/openai";  const agent = new Agent({   name: "SupportAgent",   instructions: "You are a helpful support agent.",   model: openai("gpt-4o"),   memory: new Memory(), });

Recall configuration

The three main parameters that control semantic recall behavior are:

    topK: How many semantically similar messages to retrieve
    messageRange: How much surrounding context to include with each match
    scope: Whether to search within the current thread or across all threads owned by a resource. Using scope: 'resource' allows the agent to recall information from any of the user’s past conversations.

const agent = new Agent({   memory: new Memory({     options: {       semanticRecall: {         topK: 3, // Retrieve 3 most similar messages         messageRange: 2, // Include 2 messages before and after each match         scope: 'resource', // Search across all threads for this user       },     },   }), });

Note: currently, scope: 'resource' for semantic recall is supported by the following storage adapters: LibSQL, Postgres, and Upstash.
Storage configuration

Semantic recall relies on a storage and vector db to store messages and their embeddings.

import { Memory } from "@mastra/memory"; import { Agent } from "@mastra/core/agent"; import { LibSQLStore, LibSQLVector } from "@mastra/libsql";  const agent = new Agent({   memory: new Memory({     // this is the default storage db if omitted     storage: new LibSQLStore({       url: "file:./local.db",     }),     // this is the default vector db if omitted     vector: new LibSQLVector({       connectionUrl: "file:./local.db",     }),   }), });

Storage/vector code Examples:

    LibSQL
    Postgres
    Upstash

Embedder configuration

Semantic recall relies on an embedding model to convert messages into embeddings. You can specify any embedding model 

compatible with the AI SDK.

To use FastEmbed (a local embedding model), install @mastra/fastembed:

pnpm add @mastra/fastembed

Then configure it in your memory:

import { Memory } from "@mastra/memory"; import { Agent } from "@mastra/core/agent"; import { fastembed } from "@mastra/fastembed";  const agent = new Agent({   memory: new Memory({     // ... other memory options     embedder: fastembed,   }), });

Alternatively, use a different provider like OpenAI:

import { Memory } from "@mastra/memory"; import { Agent } from "@mastra/core/agent"; import { openai } from "@ai-sdk/openai";  const agent = new Agent({   memory: new Memory({     // ... other memory options     embedder: openai.embedding("text-embedding-3-small"),   }), });

Disabling

There is a performance impact to using semantic recall. New messages are converted into embeddings and used to query a vector database before new messages are sent to the LLM.

Semantic recall is enabled by default but can be disabled when not needed:

const agent = new Agent({   memory: new Memory({     options: {       semanticRecall: false,     },   }), });

You might want to disable semantic recall in scenarios like:

    When conversation history provide sufficient context for the current conversation.
    In performance-sensitive applications, like realtime two-way audio, where the added latency of creating embeddings and running vector queries is noticeable.

Viewing Recalled Messages

When tracing is enabled, any messages retrieved via semantic recall will appear in the agent’s trace output, alongside recent conversation history (if configured).

For more info on viewing message traces, see Viewing Retrieved Messages.

Working Memory

While conversation history and semantic recall help agents remember conversations, working memory allows them to maintain persistent information about users across interactions.

Think of it as the agent’s active thoughts or scratchpad – the key information they keep available about the user or task. It’s similar to how a person would naturally remember someone’s name, preferences, or important details during a conversation.

This is useful for maintaining ongoing state that’s always relevant and should always be available to the agent.

Working memory can persist at two different scopes:

    Thread-scoped (default): Memory is isolated per conversation thread
    Resource-scoped: Memory persists across all conversation threads for the same user

Important: Switching between scopes means the agent won’t see memory from the other scope - thread-scoped memory is completely separate from resource-scoped memory.
Quick Start

Here’s a minimal example of setting up an agent with working memory:

import { Agent } from "@mastra/core/agent"; import { Memory } from "@mastra/memory"; import { openai } from "@ai-sdk/openai";  // Create agent with working memory enabled const agent = new Agent({   name: "PersonalAssistant",   instructions: "You are a helpful personal assistant.",   model: openai("gpt-4o"),   memory: new Memory({     options: {       workingMemory: {         enabled: true,       },     },   }), });

How it Works

Working memory is a block of Markdown text that the agent is able to update over time to store continuously relevant information:
Memory Persistence Scopes

Working memory can operate in two different scopes, allowing you to choose how memory persists across conversations:
Thread-Scoped Memory (Default)

By default, working memory is scoped to individual conversation threads. Each thread maintains its own isolated memory:

const memory = new Memory({   storage,   options: {     workingMemory: {       enabled: true,       scope: 'thread', // Default - memory is isolated per thread       template: `# User Profile - **Name**: - **Interests**: - **Current Goal**: `,     },   }, });

Use cases:

    Different conversations about separate topics
    Temporary or session-specific information
    Workflows where each thread needs working memory but threads are ephemeral and not related to each other

Resource-Scoped Memory

Resource-scoped memory persists across all conversation threads for the same user (resourceId), enabling persistent user memory:

const memory = new Memory({   storage,   options: {     workingMemory: {       enabled: true,       scope: 'resource', // Memory persists across all user threads       template: `# User Profile - **Name**: - **Location**: - **Interests**: - **Preferences**: - **Long-term Goals**: `,     },   }, });

Use cases:

    Personal assistants that remember user preferences
    Customer service bots that maintain customer context
    Educational applications that track student progress

Usage with Agents

When using resource-scoped memory, make sure to pass the resourceId parameter:

// Resource-scoped memory requires resourceId const response = await agent.generate("Hello!", {   threadId: "conversation-123",   resourceId: "user-alice-456" // Same user across different threads });

Storage Adapter Support

Resource-scoped working memory requires specific storage adapters that support the mastra_resources table:
✅ Supported Storage Adapters

    LibSQL (@mastra/libsql)
    PostgreSQL (@mastra/pg)
    Upstash (@mastra/upstash)

Custom Templates

Templates guide the agent on what information to track and update in working memory. While a default template is used if none is provided, you’ll typically want to define a custom template tailored to your agent’s specific use case to ensure it remembers the most relevant information.

Here’s an example of a custom template. In this example the agent will store the users name, location, timezone, etc as soon as the user sends a message containing any of the info:

const memory = new Memory({   options: {     workingMemory: {       enabled: true,       template: ` # User Profile  ## Personal Info  - Name: - Location: - Timezone:  ## Preferences  - Communication Style: [e.g., Formal, Casual] - Project Goal: - Key Deadlines:   - [Deadline 1]: [Date]   - [Deadline 2]: [Date]  ## Session State  - Last Task Discussed: - Open Questions:   - [Question 1]   - [Question 2] `,     },   }, });

Designing Effective Templates

A well-structured template keeps the information easy for the agent to parse and update. Treat the template as a short form that you want the assistant to keep up to date.

    Short, focused labels. Avoid paragraphs or very long headings. Keep labels brief (for example ## Personal Info or - Name:) so updates are easy to read and less likely to be truncated.
    Use consistent casing. Inconsistent capitalization (Timezone: vs timezone:) can cause messy updates. Stick to Title Case or lower case for headings and bullet labels.
    Keep placeholder text simple. Use hints such as [e.g., Formal] or [Date] to help the LLM fill in the correct spots.
    Abbreviate very long values. If you only need a short form, include guidance like - Name: [First name or nickname] or - Address (short): rather than the full legal text.
    Mention update rules in instructions. You can instruct how and when to fill or clear parts of the template directly in the agent’s instructions field.

Alternative Template Styles

Use a shorter single block if you only need a few items:

const basicMemory = new Memory({   options: {     workingMemory: {       enabled: true,       template: `User Facts:\n- Name:\n- Favorite Color:\n- Current Topic:`,     },   }, });

You can also store the key facts in a short paragraph format if you prefer a more narrative style:

const paragraphMemory = new Memory({   options: {     workingMemory: {       enabled: true,       template: `Important Details:\n\nKeep a short paragraph capturing the user's important facts (name, main goal, current task).`,     },   }, });

Structured Working Memory

Working memory can also be defined using a structured schema instead of a Markdown template. This allows you to specify the exact fields and types that should be tracked, using a Zod 

schema. When using a schema, the agent will see and update working memory as a JSON object matching your schema.

Important: You must specify either template or schema, but not both.
Example: Schema-Based Working Memory

import { z } from 'zod'; import { Memory } from '@mastra/memory';  const userProfileSchema = z.object({   name: z.string().optional(),   location: z.string().optional(),   timezone: z.string().optional(),   preferences: z.object({     communicationStyle: z.string().optional(),     projectGoal: z.string().optional(),     deadlines: z.array(z.string()).optional(),   }).optional(), });  const memory = new Memory({   options: {     workingMemory: {       enabled: true,       schema: userProfileSchema,       // template: ... (do not set)     },   }, });

When a schema is provided, the agent receives the working memory as a JSON object. For example:

{   "name": "Sam",   "location": "Berlin",   "timezone": "CET",   "preferences": {     "communicationStyle": "Formal",     "projectGoal": "Launch MVP",     "deadlines": ["2025-07-01"]   } }

Choosing Between Template and Schema

    Use a template (Markdown) if you want the agent to maintain memory as a free-form text block, such as a user profile or scratchpad.
    Use a schema if you need structured, type-safe data that can be validated and programmatically accessed as JSON.
    Only one mode can be active at a time: setting both template and schema is not supported.

Example: Multi-step Retention

Below is a simplified view of how the User Profile template updates across a short user conversation:

# User Profile ## Personal Info - Name: - Location: - Timezone: --- After user says "My name is **Sam** and I'm from **Berlin**" --- # User Profile - Name: Sam - Location: Berlin - Timezone: --- After user adds "By the way I'm normally in **CET**" --- # User Profile - Name: Sam - Location: Berlin - Timezone: CET

The agent can now refer to Sam or Berlin in later responses without requesting the information again because it has been stored in working memory.

If your agent is not properly updating working memory when you expect it to, you can add system instructions on how and when to use this template in your agent’s instructions setting.
Examples

    Streaming working memory
    Using a working memory template
    Using a working memory schema
    Per-resource working memory 

- Complete example showing resource-scoped memory persistence

Memory Processors

Memory Processors allow you to modify the list of messages retrieved from memory before they are added to the agent’s context window and sent to the LLM. This is useful for managing context size, filtering content, and optimizing performance.

Processors operate on the messages retrieved based on your memory configuration (e.g., lastMessages, semanticRecall). They do not affect the new incoming user message.
Built-in Processors

Mastra provides built-in processors:
TokenLimiter

This processor is used to prevent errors caused by exceeding the LLM’s context window limit. It counts the tokens in the retrieved memory messages and removes the oldest messages until the total count is below the specified limit.

import { Memory } from "@mastra/memory"; import { TokenLimiter } from "@mastra/memory/processors"; import { Agent } from "@mastra/core/agent"; import { openai } from "@ai-sdk/openai";  const agent = new Agent({   model: openai("gpt-4o"),   memory: new Memory({     processors: [       // Ensure the total tokens from memory don't exceed ~127k       new TokenLimiter(127000),     ],   }), });

The TokenLimiter uses the o200k_base encoding by default (suitable for GPT-4o). You can specify other encodings if needed for different models:

// Import the encoding you need (e.g., for older OpenAI models) import cl100k_base from "js-tiktoken/ranks/cl100k_base";  const memoryForOlderModel = new Memory({   processors: [     new TokenLimiter({       limit: 16000, // Example limit for a 16k context model       encoding: cl100k_base,     }),   ], });

See the OpenAI cookbook 

or
js-tiktoken
repo for more on encodings.
ToolCallFilter

This processor removes tool calls from the memory messages sent to the LLM. It saves tokens by excluding potentially verbose tool interactions from the context, which is useful if the details aren’t needed for future interactions. It’s also useful if you always want your agent to call a specific tool again and not rely on previous tool results in memory.

import { Memory } from "@mastra/memory"; import { ToolCallFilter, TokenLimiter } from "@mastra/memory/processors";  const memoryFilteringTools = new Memory({   processors: [     // Example 1: Remove all tool calls/results     new ToolCallFilter(),      // Example 2: Remove only noisy image generation tool calls/results     new ToolCallFilter({ exclude: ["generateImageTool"] }),      // Always place TokenLimiter last     new TokenLimiter(127000),   ], });

Applying Multiple Processors

You can chain multiple processors. They execute in the order they appear in the processors array. The output of one processor becomes the input for the next.

Order matters! It’s generally best practice to place TokenLimiter last in the chain. This ensures it operates on the final set of messages after other filtering has occurred, providing the most accurate token limit enforcement.

import { Memory } from "@mastra/memory"; import { ToolCallFilter, TokenLimiter } from "@mastra/memory/processors"; // Assume a hypothetical 'PIIFilter' custom processor exists // import { PIIFilter } from './custom-processors';  const memoryWithMultipleProcessors = new Memory({   processors: [     // 1. Filter specific tool calls first     new ToolCallFilter({ exclude: ["verboseDebugTool"] }),     // 2. Apply custom filtering (e.g., remove hypothetical PII - use with caution)     // new PIIFilter(),     // 3. Apply token limiting as the final step     new TokenLimiter(127000),   ], });

Creating Custom Processors

You can create custom logic by extending the base MemoryProcessor class.

import { Memory, CoreMessage } from "@mastra/memory"; import { MemoryProcessor, MemoryProcessorOpts } from "@mastra/core/memory";  class ConversationOnlyFilter extends MemoryProcessor {   constructor() {     // Provide a name for easier debugging if needed     super({ name: "ConversationOnlyFilter" });   }    process(     messages: CoreMessage[],     _opts: MemoryProcessorOpts = {}, // Options passed during memory retrieval, rarely needed here   ): CoreMessage[] {     // Filter messages based on role     return messages.filter(       (msg) => msg.role === "user" || msg.role === "assistant",     );   } }  // Use the custom processor const memoryWithCustomFilter = new Memory({   processors: [     new ConversationOnlyFilter(),     new TokenLimiter(127000), // Still apply token limiting   ], });

When creating custom processors avoid mutating the input messages array or its objects directly.